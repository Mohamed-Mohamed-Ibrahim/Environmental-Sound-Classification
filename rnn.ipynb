{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12092109,"sourceType":"datasetVersion","datasetId":7612161}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pickle\n\nimport time\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\n# from name_dataset import NameDataset\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:56:51.603158Z","iopub.execute_input":"2025-06-09T19:56:51.603880Z","iopub.status.idle":"2025-06-09T19:56:55.902094Z","shell.execute_reply.started":"2025-06-09T19:56:51.603822Z","shell.execute_reply":"2025-06-09T19:56:55.901229Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/urbansound8k-feature-extraction/train_data.pkl', 'rb') as f:\n    X_train, Y_train = pickle.load(f)\n\nwith open('/kaggle/input/urbansound8k-feature-extraction/val_data.pkl', 'rb') as f:\n    X_val, Y_val = pickle.load(f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:56:55.903423Z","iopub.execute_input":"2025-06-09T19:56:55.903734Z","iopub.status.idle":"2025-06-09T19:57:09.707386Z","shell.execute_reply.started":"2025-06-09T19:56:55.903715Z","shell.execute_reply":"2025-06-09T19:57:09.706867Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef stack_samples(X, Y):\n    \"\"\"\n    X: list or array of shape (N, R, C)\n    Y: list of N labels\n    Returns: DataFrame of shape (N×R, C + 1)\n    \"\"\"\n    X_np = np.array(X, dtype=np.float32)  # Shape: (N, R, C)\n    Y_repeated = np.repeat(Y, X_np.shape[1])  # Repeat each label R times\n\n    X_stacked = X_np.reshape(-1, X_np.shape[2])  # (N×R, C)\n\n    df = pd.DataFrame(X_stacked)\n    df['labels'] = Y_repeated\n\n    return df\n\ntrain_df = stack_samples(X_train, Y_train)\nval_df = stack_samples(X_val, Y_val)\nprint(train_df.shape)\nprint(val_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:57:09.708054Z","iopub.execute_input":"2025-06-09T19:57:09.708234Z","iopub.status.idle":"2025-06-09T19:57:10.703457Z","shell.execute_reply.started":"2025-06-09T19:57:09.708218Z","shell.execute_reply":"2025-06-09T19:57:10.702885Z"}},"outputs":[{"name":"stdout","text":"(1173960, 181)\n(355104, 181)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train_df.head(54)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:57:10.705185Z","iopub.execute_input":"2025-06-09T19:57:10.705501Z","iopub.status.idle":"2025-06-09T19:57:10.764529Z","shell.execute_reply.started":"2025-06-09T19:57:10.705481Z","shell.execute_reply":"2025-06-09T19:57:10.763870Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"            0          1          2          3          4          5  \\\n0    0.027114   0.069684   0.084108   0.086272   0.086115   0.068998   \n1    0.430208   5.758408  51.152721  60.381077  21.515299   4.466881   \n2    0.356129   2.768868  10.043193   5.306677   2.211567   2.200262   \n3    0.119155   0.518129   1.104150   0.707675   0.361656   0.435153   \n4    0.200908   0.502827   0.919906   0.679136   0.617777   0.499163   \n5    0.170222   0.579005   0.849931   0.902989   0.607756   0.673738   \n6    0.110933   0.318491   1.234081   1.249484   1.085954   0.533614   \n7    0.164980   0.210829   0.551717   1.482775   2.704562   0.968102   \n8    0.080657   0.206064   0.383828   0.475435   0.290516   0.347667   \n9    0.067881   0.175669   0.380467   0.296372   0.141358   0.099105   \n10   0.049857   0.155923   0.512764   0.595865   0.232287   0.150851   \n11   0.025313   0.094191   0.371903   0.567046   0.323835   0.164010   \n12   0.045070   0.062929   0.288921   0.435305   0.188783   0.153368   \n13   0.167366   0.222917   0.535716   0.614709   0.482779   0.430796   \n14   0.080346   0.132293   0.690539   0.959159   0.610840   0.311777   \n15   0.405447   0.440197   1.374574   2.650306   1.921984   1.651224   \n16   0.252143   0.358236   0.687975   0.828957   0.827738   0.780572   \n17   0.048770   0.125692   0.126118   0.193281   0.327594   0.247377   \n18   0.067230   0.110967   0.113277   0.168346   0.384789   0.302926   \n19   0.029880   0.046652   0.062196   0.087927   0.195913   0.137761   \n20   0.032038   0.044129   0.104852   0.161041   0.118781   0.085848   \n21   0.058707   0.080883   0.134095   0.282596   0.196937   0.119976   \n22   0.017250   0.044472   0.082715   0.127770   0.098439   0.053567   \n23   0.020888   0.053215   0.188061   0.303666   0.190056   0.123566   \n24   0.034141   0.097988   0.146446   0.223052   0.212820   0.131226   \n25   0.016999   0.074886   0.279075   0.342038   0.267576   0.164769   \n26   0.067098   0.130628   0.503974   0.743972   0.482521   0.201073   \n27   0.072534   0.101249   0.367012   0.486975   0.256208   0.144141   \n28   0.045395   0.060836   0.077814   0.194641   0.173909   0.053511   \n29   0.073125   0.122621   0.260987   0.646580   0.401492   0.120446   \n30   0.023342   0.046804   0.093730   0.185015   0.128838   0.053166   \n31   0.014442   0.027781   0.076716   0.097175   0.100682   0.063224   \n32   0.013831   0.029115   0.085920   0.107735   0.126828   0.083698   \n33   0.012840   0.021401   0.048260   0.047572   0.022251   0.013919   \n34   0.006640   0.010976   0.020232   0.026340   0.020650   0.014114   \n35   0.006299   0.011649   0.017960   0.021299   0.013808   0.008313   \n36   0.008149   0.012901   0.021439   0.026773   0.021312   0.010636   \n37   0.007141   0.012627   0.034754   0.032717   0.019491   0.012873   \n38   0.005576   0.009728   0.026462   0.026470   0.014545   0.009987   \n39   0.001436   0.003678   0.012229   0.011120   0.006273   0.003391   \n40   0.000461   0.001861   0.005560   0.004666   0.002392   0.001385   \n41 -89.786438 -67.908806 -43.376614 -36.558353 -45.194103 -58.265862   \n42  32.098133  36.828247  38.836082  37.278412  36.932884  39.667648   \n43  -7.981393  -2.199342   0.669037  -5.068494  -9.907617  -9.968912   \n44  10.927966  15.592958  16.145491  16.776062  15.926103  12.866306   \n45  -4.331727   0.045187  -1.214716  -3.860799  -2.713581  -1.429191   \n46  10.103023   9.045020   7.031740   6.609464   9.399591  10.392502   \n47   5.360611   8.094102  13.553589  12.842154   8.851343   6.926183   \n48  -1.699639   1.444747   5.796215   4.132884  -1.267477  -0.937314   \n49  -7.738241  -4.367334  -2.147584  -2.027264  -4.970896  -6.674585   \n50   3.241025   3.917500   2.288239   2.579389   2.439355   0.555137   \n51  -2.698967   2.360949   4.066176   2.438357   3.026912   3.466618   \n52   7.517341   6.866739   7.780427  10.452810   9.378583   6.908299   \n53   2.431201   0.125495   3.022161   4.687483   3.114458   0.597315   \n\n            6          7          8          9  ...        171        172  \\\n0    0.075101   0.081550   0.082987   0.075951  ...   0.078393   0.053319   \n1   13.074950  55.018288  43.365993  16.410397  ...  25.460247   5.669981   \n2    5.633092  11.511503   4.667964   2.912486  ...   5.367310   2.295357   \n3    0.531613   1.172129   1.274063   1.586210  ...   1.393059   0.565431   \n4    0.740346   0.879904   0.768322   0.788436  ...   1.237146   0.505317   \n5    0.697245   1.517281   1.158525   0.456944  ...   0.783423   0.195463   \n6    0.550653   1.347195   0.960807   0.647482  ...   0.694076   0.386585   \n7    0.979872   1.822335   2.469921   1.716560  ...   1.440140   0.662305   \n8    0.401557   0.537649   0.564168   0.395445  ...   1.213662   0.555193   \n9    0.360744   0.758478   0.346240   0.308535  ...   0.190039   0.234406   \n10   0.452648   1.125487   0.464845   0.252408  ...   0.670935   0.289519   \n11   0.300381   1.641869   1.457296   0.528285  ...   1.787701   0.948824   \n12   0.150651   0.382667   0.481892   0.170566  ...   0.786378   0.254148   \n13   0.189239   0.400604   0.588328   0.297422  ...   0.382157   0.202374   \n14   0.279917   0.771439   0.628605   0.353577  ...   0.328883   0.250781   \n15   1.089859   1.391183   1.466707   0.514634  ...   0.396426   0.289298   \n16   0.641913   0.602526   0.748944   0.500214  ...   0.456070   0.324299   \n17   0.158699   0.221894   0.269632   0.163622  ...   0.574288   0.403611   \n18   0.163020   0.190071   0.231270   0.174362  ...   0.231285   0.130500   \n19   0.099709   0.308271   0.280520   0.267784  ...   0.067154   0.057476   \n20   0.046733   0.114507   0.164914   0.169560  ...   0.086647   0.059092   \n21   0.053122   0.103321   0.166946   0.163954  ...   0.086724   0.037147   \n22   0.074232   0.154927   0.190292   0.139016  ...   0.083700   0.033489   \n23   0.077306   0.193364   0.193696   0.160445  ...   0.086592   0.044095   \n24   0.098599   0.123115   0.111673   0.108152  ...   0.097596   0.052166   \n25   0.146835   0.211414   0.279076   0.218051  ...   0.310759   0.200979   \n26   0.114103   0.329139   0.387518   0.241363  ...   0.391188   0.196845   \n27   0.075962   0.227654   0.247352   0.158116  ...   0.094818   0.037829   \n28   0.029939   0.087512   0.145436   0.159231  ...   0.104368   0.034591   \n29   0.107945   0.346116   0.439682   0.194681  ...   0.138710   0.057082   \n30   0.064028   0.128264   0.151120   0.112333  ...   0.062858   0.043057   \n31   0.044449   0.103382   0.093354   0.057577  ...   0.062073   0.028864   \n32   0.057174   0.122874   0.104177   0.086112  ...   0.029635   0.011777   \n33   0.014792   0.033550   0.036727   0.025119  ...   0.030523   0.020679   \n34   0.012607   0.019920   0.020722   0.021085  ...   0.021342   0.012693   \n35   0.012282   0.036412   0.034095   0.023049  ...   0.014192   0.009777   \n36   0.012200   0.036329   0.038201   0.024106  ...   0.013417   0.009240   \n37   0.012930   0.028296   0.027071   0.018305  ...   0.007092   0.004893   \n38   0.009933   0.026031   0.020805   0.014679  ...   0.004097   0.002373   \n39   0.006737   0.015215   0.011277   0.007927  ...   0.003579   0.001536   \n40   0.002803   0.006090   0.003910   0.001853  ...   0.001049   0.000540   \n41 -58.036423 -36.861099 -37.570499 -48.556938  ... -50.513130 -68.605568   \n42  43.256218  42.125481  40.193859  38.543259  ...  49.322006  47.465416   \n43  -0.850861   1.021164  -3.592535  -4.686235  ...  -4.714941  -6.918634   \n44  11.797743  12.803131  13.731473  16.505934  ...  16.156738  12.169458   \n45  -1.328908  -2.853686  -3.055339  -0.796875  ...  -6.579997  -7.191135   \n46   7.861430   6.779309   7.973168   7.687958  ...   7.694796   8.326796   \n47   7.417639   8.807932   8.522857   4.878817  ...   7.556949   5.528845   \n48   1.569834   3.854502   3.062372   2.332791  ...   5.579550   5.321700   \n49  -2.228589   1.361968  -0.413936  -0.812683  ...  -1.936213  -3.773699   \n50   2.917523   5.263381   4.766595   5.322254  ...   6.697255   8.117208   \n51   7.097373   3.498999   0.728380   0.712621  ...   3.984136   5.532791   \n52   8.017244   7.424052   7.822616   7.144040  ...   2.216845   3.029710   \n53  -1.188038   0.630533   2.249921   1.931519  ...  -0.580042  -1.383816   \n\n    173  174  175  176  177  178  179      labels  \n0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n5   0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n6   0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n8   0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n9   0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n10  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n11  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n12  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n13  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n14  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n15  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n16  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n17  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n18  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n19  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n20  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n21  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n22  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n23  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n24  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n25  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n26  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n27  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n28  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n29  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n30  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n31  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n32  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n33  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n34  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n35  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n36  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n37  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n38  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n39  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n40  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n41  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n42  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n43  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n44  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n45  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n46  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n47  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n48  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n49  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n50  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n51  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n52  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n53  0.0  0.0  0.0  0.0  0.0  0.0  0.0  jackhammer  \n\n[54 rows x 181 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>171</th>\n      <th>172</th>\n      <th>173</th>\n      <th>174</th>\n      <th>175</th>\n      <th>176</th>\n      <th>177</th>\n      <th>178</th>\n      <th>179</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.027114</td>\n      <td>0.069684</td>\n      <td>0.084108</td>\n      <td>0.086272</td>\n      <td>0.086115</td>\n      <td>0.068998</td>\n      <td>0.075101</td>\n      <td>0.081550</td>\n      <td>0.082987</td>\n      <td>0.075951</td>\n      <td>...</td>\n      <td>0.078393</td>\n      <td>0.053319</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.430208</td>\n      <td>5.758408</td>\n      <td>51.152721</td>\n      <td>60.381077</td>\n      <td>21.515299</td>\n      <td>4.466881</td>\n      <td>13.074950</td>\n      <td>55.018288</td>\n      <td>43.365993</td>\n      <td>16.410397</td>\n      <td>...</td>\n      <td>25.460247</td>\n      <td>5.669981</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.356129</td>\n      <td>2.768868</td>\n      <td>10.043193</td>\n      <td>5.306677</td>\n      <td>2.211567</td>\n      <td>2.200262</td>\n      <td>5.633092</td>\n      <td>11.511503</td>\n      <td>4.667964</td>\n      <td>2.912486</td>\n      <td>...</td>\n      <td>5.367310</td>\n      <td>2.295357</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.119155</td>\n      <td>0.518129</td>\n      <td>1.104150</td>\n      <td>0.707675</td>\n      <td>0.361656</td>\n      <td>0.435153</td>\n      <td>0.531613</td>\n      <td>1.172129</td>\n      <td>1.274063</td>\n      <td>1.586210</td>\n      <td>...</td>\n      <td>1.393059</td>\n      <td>0.565431</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.200908</td>\n      <td>0.502827</td>\n      <td>0.919906</td>\n      <td>0.679136</td>\n      <td>0.617777</td>\n      <td>0.499163</td>\n      <td>0.740346</td>\n      <td>0.879904</td>\n      <td>0.768322</td>\n      <td>0.788436</td>\n      <td>...</td>\n      <td>1.237146</td>\n      <td>0.505317</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.170222</td>\n      <td>0.579005</td>\n      <td>0.849931</td>\n      <td>0.902989</td>\n      <td>0.607756</td>\n      <td>0.673738</td>\n      <td>0.697245</td>\n      <td>1.517281</td>\n      <td>1.158525</td>\n      <td>0.456944</td>\n      <td>...</td>\n      <td>0.783423</td>\n      <td>0.195463</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.110933</td>\n      <td>0.318491</td>\n      <td>1.234081</td>\n      <td>1.249484</td>\n      <td>1.085954</td>\n      <td>0.533614</td>\n      <td>0.550653</td>\n      <td>1.347195</td>\n      <td>0.960807</td>\n      <td>0.647482</td>\n      <td>...</td>\n      <td>0.694076</td>\n      <td>0.386585</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.164980</td>\n      <td>0.210829</td>\n      <td>0.551717</td>\n      <td>1.482775</td>\n      <td>2.704562</td>\n      <td>0.968102</td>\n      <td>0.979872</td>\n      <td>1.822335</td>\n      <td>2.469921</td>\n      <td>1.716560</td>\n      <td>...</td>\n      <td>1.440140</td>\n      <td>0.662305</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.080657</td>\n      <td>0.206064</td>\n      <td>0.383828</td>\n      <td>0.475435</td>\n      <td>0.290516</td>\n      <td>0.347667</td>\n      <td>0.401557</td>\n      <td>0.537649</td>\n      <td>0.564168</td>\n      <td>0.395445</td>\n      <td>...</td>\n      <td>1.213662</td>\n      <td>0.555193</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.067881</td>\n      <td>0.175669</td>\n      <td>0.380467</td>\n      <td>0.296372</td>\n      <td>0.141358</td>\n      <td>0.099105</td>\n      <td>0.360744</td>\n      <td>0.758478</td>\n      <td>0.346240</td>\n      <td>0.308535</td>\n      <td>...</td>\n      <td>0.190039</td>\n      <td>0.234406</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.049857</td>\n      <td>0.155923</td>\n      <td>0.512764</td>\n      <td>0.595865</td>\n      <td>0.232287</td>\n      <td>0.150851</td>\n      <td>0.452648</td>\n      <td>1.125487</td>\n      <td>0.464845</td>\n      <td>0.252408</td>\n      <td>...</td>\n      <td>0.670935</td>\n      <td>0.289519</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.025313</td>\n      <td>0.094191</td>\n      <td>0.371903</td>\n      <td>0.567046</td>\n      <td>0.323835</td>\n      <td>0.164010</td>\n      <td>0.300381</td>\n      <td>1.641869</td>\n      <td>1.457296</td>\n      <td>0.528285</td>\n      <td>...</td>\n      <td>1.787701</td>\n      <td>0.948824</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.045070</td>\n      <td>0.062929</td>\n      <td>0.288921</td>\n      <td>0.435305</td>\n      <td>0.188783</td>\n      <td>0.153368</td>\n      <td>0.150651</td>\n      <td>0.382667</td>\n      <td>0.481892</td>\n      <td>0.170566</td>\n      <td>...</td>\n      <td>0.786378</td>\n      <td>0.254148</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.167366</td>\n      <td>0.222917</td>\n      <td>0.535716</td>\n      <td>0.614709</td>\n      <td>0.482779</td>\n      <td>0.430796</td>\n      <td>0.189239</td>\n      <td>0.400604</td>\n      <td>0.588328</td>\n      <td>0.297422</td>\n      <td>...</td>\n      <td>0.382157</td>\n      <td>0.202374</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.080346</td>\n      <td>0.132293</td>\n      <td>0.690539</td>\n      <td>0.959159</td>\n      <td>0.610840</td>\n      <td>0.311777</td>\n      <td>0.279917</td>\n      <td>0.771439</td>\n      <td>0.628605</td>\n      <td>0.353577</td>\n      <td>...</td>\n      <td>0.328883</td>\n      <td>0.250781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.405447</td>\n      <td>0.440197</td>\n      <td>1.374574</td>\n      <td>2.650306</td>\n      <td>1.921984</td>\n      <td>1.651224</td>\n      <td>1.089859</td>\n      <td>1.391183</td>\n      <td>1.466707</td>\n      <td>0.514634</td>\n      <td>...</td>\n      <td>0.396426</td>\n      <td>0.289298</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.252143</td>\n      <td>0.358236</td>\n      <td>0.687975</td>\n      <td>0.828957</td>\n      <td>0.827738</td>\n      <td>0.780572</td>\n      <td>0.641913</td>\n      <td>0.602526</td>\n      <td>0.748944</td>\n      <td>0.500214</td>\n      <td>...</td>\n      <td>0.456070</td>\n      <td>0.324299</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.048770</td>\n      <td>0.125692</td>\n      <td>0.126118</td>\n      <td>0.193281</td>\n      <td>0.327594</td>\n      <td>0.247377</td>\n      <td>0.158699</td>\n      <td>0.221894</td>\n      <td>0.269632</td>\n      <td>0.163622</td>\n      <td>...</td>\n      <td>0.574288</td>\n      <td>0.403611</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.067230</td>\n      <td>0.110967</td>\n      <td>0.113277</td>\n      <td>0.168346</td>\n      <td>0.384789</td>\n      <td>0.302926</td>\n      <td>0.163020</td>\n      <td>0.190071</td>\n      <td>0.231270</td>\n      <td>0.174362</td>\n      <td>...</td>\n      <td>0.231285</td>\n      <td>0.130500</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.029880</td>\n      <td>0.046652</td>\n      <td>0.062196</td>\n      <td>0.087927</td>\n      <td>0.195913</td>\n      <td>0.137761</td>\n      <td>0.099709</td>\n      <td>0.308271</td>\n      <td>0.280520</td>\n      <td>0.267784</td>\n      <td>...</td>\n      <td>0.067154</td>\n      <td>0.057476</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.032038</td>\n      <td>0.044129</td>\n      <td>0.104852</td>\n      <td>0.161041</td>\n      <td>0.118781</td>\n      <td>0.085848</td>\n      <td>0.046733</td>\n      <td>0.114507</td>\n      <td>0.164914</td>\n      <td>0.169560</td>\n      <td>...</td>\n      <td>0.086647</td>\n      <td>0.059092</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.058707</td>\n      <td>0.080883</td>\n      <td>0.134095</td>\n      <td>0.282596</td>\n      <td>0.196937</td>\n      <td>0.119976</td>\n      <td>0.053122</td>\n      <td>0.103321</td>\n      <td>0.166946</td>\n      <td>0.163954</td>\n      <td>...</td>\n      <td>0.086724</td>\n      <td>0.037147</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.017250</td>\n      <td>0.044472</td>\n      <td>0.082715</td>\n      <td>0.127770</td>\n      <td>0.098439</td>\n      <td>0.053567</td>\n      <td>0.074232</td>\n      <td>0.154927</td>\n      <td>0.190292</td>\n      <td>0.139016</td>\n      <td>...</td>\n      <td>0.083700</td>\n      <td>0.033489</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.020888</td>\n      <td>0.053215</td>\n      <td>0.188061</td>\n      <td>0.303666</td>\n      <td>0.190056</td>\n      <td>0.123566</td>\n      <td>0.077306</td>\n      <td>0.193364</td>\n      <td>0.193696</td>\n      <td>0.160445</td>\n      <td>...</td>\n      <td>0.086592</td>\n      <td>0.044095</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.034141</td>\n      <td>0.097988</td>\n      <td>0.146446</td>\n      <td>0.223052</td>\n      <td>0.212820</td>\n      <td>0.131226</td>\n      <td>0.098599</td>\n      <td>0.123115</td>\n      <td>0.111673</td>\n      <td>0.108152</td>\n      <td>...</td>\n      <td>0.097596</td>\n      <td>0.052166</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.016999</td>\n      <td>0.074886</td>\n      <td>0.279075</td>\n      <td>0.342038</td>\n      <td>0.267576</td>\n      <td>0.164769</td>\n      <td>0.146835</td>\n      <td>0.211414</td>\n      <td>0.279076</td>\n      <td>0.218051</td>\n      <td>...</td>\n      <td>0.310759</td>\n      <td>0.200979</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.067098</td>\n      <td>0.130628</td>\n      <td>0.503974</td>\n      <td>0.743972</td>\n      <td>0.482521</td>\n      <td>0.201073</td>\n      <td>0.114103</td>\n      <td>0.329139</td>\n      <td>0.387518</td>\n      <td>0.241363</td>\n      <td>...</td>\n      <td>0.391188</td>\n      <td>0.196845</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.072534</td>\n      <td>0.101249</td>\n      <td>0.367012</td>\n      <td>0.486975</td>\n      <td>0.256208</td>\n      <td>0.144141</td>\n      <td>0.075962</td>\n      <td>0.227654</td>\n      <td>0.247352</td>\n      <td>0.158116</td>\n      <td>...</td>\n      <td>0.094818</td>\n      <td>0.037829</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.045395</td>\n      <td>0.060836</td>\n      <td>0.077814</td>\n      <td>0.194641</td>\n      <td>0.173909</td>\n      <td>0.053511</td>\n      <td>0.029939</td>\n      <td>0.087512</td>\n      <td>0.145436</td>\n      <td>0.159231</td>\n      <td>...</td>\n      <td>0.104368</td>\n      <td>0.034591</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.073125</td>\n      <td>0.122621</td>\n      <td>0.260987</td>\n      <td>0.646580</td>\n      <td>0.401492</td>\n      <td>0.120446</td>\n      <td>0.107945</td>\n      <td>0.346116</td>\n      <td>0.439682</td>\n      <td>0.194681</td>\n      <td>...</td>\n      <td>0.138710</td>\n      <td>0.057082</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.023342</td>\n      <td>0.046804</td>\n      <td>0.093730</td>\n      <td>0.185015</td>\n      <td>0.128838</td>\n      <td>0.053166</td>\n      <td>0.064028</td>\n      <td>0.128264</td>\n      <td>0.151120</td>\n      <td>0.112333</td>\n      <td>...</td>\n      <td>0.062858</td>\n      <td>0.043057</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.014442</td>\n      <td>0.027781</td>\n      <td>0.076716</td>\n      <td>0.097175</td>\n      <td>0.100682</td>\n      <td>0.063224</td>\n      <td>0.044449</td>\n      <td>0.103382</td>\n      <td>0.093354</td>\n      <td>0.057577</td>\n      <td>...</td>\n      <td>0.062073</td>\n      <td>0.028864</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.013831</td>\n      <td>0.029115</td>\n      <td>0.085920</td>\n      <td>0.107735</td>\n      <td>0.126828</td>\n      <td>0.083698</td>\n      <td>0.057174</td>\n      <td>0.122874</td>\n      <td>0.104177</td>\n      <td>0.086112</td>\n      <td>...</td>\n      <td>0.029635</td>\n      <td>0.011777</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.012840</td>\n      <td>0.021401</td>\n      <td>0.048260</td>\n      <td>0.047572</td>\n      <td>0.022251</td>\n      <td>0.013919</td>\n      <td>0.014792</td>\n      <td>0.033550</td>\n      <td>0.036727</td>\n      <td>0.025119</td>\n      <td>...</td>\n      <td>0.030523</td>\n      <td>0.020679</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.006640</td>\n      <td>0.010976</td>\n      <td>0.020232</td>\n      <td>0.026340</td>\n      <td>0.020650</td>\n      <td>0.014114</td>\n      <td>0.012607</td>\n      <td>0.019920</td>\n      <td>0.020722</td>\n      <td>0.021085</td>\n      <td>...</td>\n      <td>0.021342</td>\n      <td>0.012693</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.006299</td>\n      <td>0.011649</td>\n      <td>0.017960</td>\n      <td>0.021299</td>\n      <td>0.013808</td>\n      <td>0.008313</td>\n      <td>0.012282</td>\n      <td>0.036412</td>\n      <td>0.034095</td>\n      <td>0.023049</td>\n      <td>...</td>\n      <td>0.014192</td>\n      <td>0.009777</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.008149</td>\n      <td>0.012901</td>\n      <td>0.021439</td>\n      <td>0.026773</td>\n      <td>0.021312</td>\n      <td>0.010636</td>\n      <td>0.012200</td>\n      <td>0.036329</td>\n      <td>0.038201</td>\n      <td>0.024106</td>\n      <td>...</td>\n      <td>0.013417</td>\n      <td>0.009240</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.007141</td>\n      <td>0.012627</td>\n      <td>0.034754</td>\n      <td>0.032717</td>\n      <td>0.019491</td>\n      <td>0.012873</td>\n      <td>0.012930</td>\n      <td>0.028296</td>\n      <td>0.027071</td>\n      <td>0.018305</td>\n      <td>...</td>\n      <td>0.007092</td>\n      <td>0.004893</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.005576</td>\n      <td>0.009728</td>\n      <td>0.026462</td>\n      <td>0.026470</td>\n      <td>0.014545</td>\n      <td>0.009987</td>\n      <td>0.009933</td>\n      <td>0.026031</td>\n      <td>0.020805</td>\n      <td>0.014679</td>\n      <td>...</td>\n      <td>0.004097</td>\n      <td>0.002373</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.001436</td>\n      <td>0.003678</td>\n      <td>0.012229</td>\n      <td>0.011120</td>\n      <td>0.006273</td>\n      <td>0.003391</td>\n      <td>0.006737</td>\n      <td>0.015215</td>\n      <td>0.011277</td>\n      <td>0.007927</td>\n      <td>...</td>\n      <td>0.003579</td>\n      <td>0.001536</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.000461</td>\n      <td>0.001861</td>\n      <td>0.005560</td>\n      <td>0.004666</td>\n      <td>0.002392</td>\n      <td>0.001385</td>\n      <td>0.002803</td>\n      <td>0.006090</td>\n      <td>0.003910</td>\n      <td>0.001853</td>\n      <td>...</td>\n      <td>0.001049</td>\n      <td>0.000540</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>-89.786438</td>\n      <td>-67.908806</td>\n      <td>-43.376614</td>\n      <td>-36.558353</td>\n      <td>-45.194103</td>\n      <td>-58.265862</td>\n      <td>-58.036423</td>\n      <td>-36.861099</td>\n      <td>-37.570499</td>\n      <td>-48.556938</td>\n      <td>...</td>\n      <td>-50.513130</td>\n      <td>-68.605568</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>32.098133</td>\n      <td>36.828247</td>\n      <td>38.836082</td>\n      <td>37.278412</td>\n      <td>36.932884</td>\n      <td>39.667648</td>\n      <td>43.256218</td>\n      <td>42.125481</td>\n      <td>40.193859</td>\n      <td>38.543259</td>\n      <td>...</td>\n      <td>49.322006</td>\n      <td>47.465416</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>-7.981393</td>\n      <td>-2.199342</td>\n      <td>0.669037</td>\n      <td>-5.068494</td>\n      <td>-9.907617</td>\n      <td>-9.968912</td>\n      <td>-0.850861</td>\n      <td>1.021164</td>\n      <td>-3.592535</td>\n      <td>-4.686235</td>\n      <td>...</td>\n      <td>-4.714941</td>\n      <td>-6.918634</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>10.927966</td>\n      <td>15.592958</td>\n      <td>16.145491</td>\n      <td>16.776062</td>\n      <td>15.926103</td>\n      <td>12.866306</td>\n      <td>11.797743</td>\n      <td>12.803131</td>\n      <td>13.731473</td>\n      <td>16.505934</td>\n      <td>...</td>\n      <td>16.156738</td>\n      <td>12.169458</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>-4.331727</td>\n      <td>0.045187</td>\n      <td>-1.214716</td>\n      <td>-3.860799</td>\n      <td>-2.713581</td>\n      <td>-1.429191</td>\n      <td>-1.328908</td>\n      <td>-2.853686</td>\n      <td>-3.055339</td>\n      <td>-0.796875</td>\n      <td>...</td>\n      <td>-6.579997</td>\n      <td>-7.191135</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>10.103023</td>\n      <td>9.045020</td>\n      <td>7.031740</td>\n      <td>6.609464</td>\n      <td>9.399591</td>\n      <td>10.392502</td>\n      <td>7.861430</td>\n      <td>6.779309</td>\n      <td>7.973168</td>\n      <td>7.687958</td>\n      <td>...</td>\n      <td>7.694796</td>\n      <td>8.326796</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>5.360611</td>\n      <td>8.094102</td>\n      <td>13.553589</td>\n      <td>12.842154</td>\n      <td>8.851343</td>\n      <td>6.926183</td>\n      <td>7.417639</td>\n      <td>8.807932</td>\n      <td>8.522857</td>\n      <td>4.878817</td>\n      <td>...</td>\n      <td>7.556949</td>\n      <td>5.528845</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>-1.699639</td>\n      <td>1.444747</td>\n      <td>5.796215</td>\n      <td>4.132884</td>\n      <td>-1.267477</td>\n      <td>-0.937314</td>\n      <td>1.569834</td>\n      <td>3.854502</td>\n      <td>3.062372</td>\n      <td>2.332791</td>\n      <td>...</td>\n      <td>5.579550</td>\n      <td>5.321700</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>-7.738241</td>\n      <td>-4.367334</td>\n      <td>-2.147584</td>\n      <td>-2.027264</td>\n      <td>-4.970896</td>\n      <td>-6.674585</td>\n      <td>-2.228589</td>\n      <td>1.361968</td>\n      <td>-0.413936</td>\n      <td>-0.812683</td>\n      <td>...</td>\n      <td>-1.936213</td>\n      <td>-3.773699</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>3.241025</td>\n      <td>3.917500</td>\n      <td>2.288239</td>\n      <td>2.579389</td>\n      <td>2.439355</td>\n      <td>0.555137</td>\n      <td>2.917523</td>\n      <td>5.263381</td>\n      <td>4.766595</td>\n      <td>5.322254</td>\n      <td>...</td>\n      <td>6.697255</td>\n      <td>8.117208</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>-2.698967</td>\n      <td>2.360949</td>\n      <td>4.066176</td>\n      <td>2.438357</td>\n      <td>3.026912</td>\n      <td>3.466618</td>\n      <td>7.097373</td>\n      <td>3.498999</td>\n      <td>0.728380</td>\n      <td>0.712621</td>\n      <td>...</td>\n      <td>3.984136</td>\n      <td>5.532791</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>7.517341</td>\n      <td>6.866739</td>\n      <td>7.780427</td>\n      <td>10.452810</td>\n      <td>9.378583</td>\n      <td>6.908299</td>\n      <td>8.017244</td>\n      <td>7.424052</td>\n      <td>7.822616</td>\n      <td>7.144040</td>\n      <td>...</td>\n      <td>2.216845</td>\n      <td>3.029710</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>2.431201</td>\n      <td>0.125495</td>\n      <td>3.022161</td>\n      <td>4.687483</td>\n      <td>3.114458</td>\n      <td>0.597315</td>\n      <td>-1.188038</td>\n      <td>0.630533</td>\n      <td>2.249921</td>\n      <td>1.931519</td>\n      <td>...</td>\n      <td>-0.580042</td>\n      <td>-1.383816</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n  </tbody>\n</table>\n<p>54 rows × 181 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"val_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:57:10.765159Z","iopub.execute_input":"2025-06-09T19:57:10.765342Z","iopub.status.idle":"2025-06-09T19:57:10.782252Z","shell.execute_reply.started":"2025-06-09T19:57:10.765327Z","shell.execute_reply":"2025-06-09T19:57:10.781623Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"          0         1          2         3          4          5         6  \\\n0  0.044180  0.056054   0.064415  0.069324   0.067334   0.067759  0.071800   \n1  1.723533  5.687970   9.228014  7.410962   7.198935  11.340713  7.369151   \n2  2.469047  7.855614  10.789010  9.705440  11.964504  14.177218  9.775521   \n3  2.648386  5.412858   5.202138  6.334105  10.531077   8.599378  4.707143   \n4  0.576799  1.618431   2.621299  1.711736   1.985173   2.103726  1.531987   \n\n           7          8         9  ...        171        172  173  174  175  \\\n0   0.069728   0.069248  0.065749  ...   0.070372   0.061280  0.0  0.0  0.0   \n1  10.589535  10.131034  3.039403  ...  14.573446  10.166873  0.0  0.0  0.0   \n2  12.749818  11.203400  4.147494  ...  12.323687   5.084070  0.0  0.0  0.0   \n3   6.276899   5.755446  8.934631  ...  10.442652   4.127263  0.0  0.0  0.0   \n4   2.888803   3.074646  3.813351  ...   4.061364   3.374841  0.0  0.0  0.0   \n\n   176  177  178  179      labels  \n0  0.0  0.0  0.0  0.0  jackhammer  \n1  0.0  0.0  0.0  0.0  jackhammer  \n2  0.0  0.0  0.0  0.0  jackhammer  \n3  0.0  0.0  0.0  0.0  jackhammer  \n4  0.0  0.0  0.0  0.0  jackhammer  \n\n[5 rows x 181 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>171</th>\n      <th>172</th>\n      <th>173</th>\n      <th>174</th>\n      <th>175</th>\n      <th>176</th>\n      <th>177</th>\n      <th>178</th>\n      <th>179</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.044180</td>\n      <td>0.056054</td>\n      <td>0.064415</td>\n      <td>0.069324</td>\n      <td>0.067334</td>\n      <td>0.067759</td>\n      <td>0.071800</td>\n      <td>0.069728</td>\n      <td>0.069248</td>\n      <td>0.065749</td>\n      <td>...</td>\n      <td>0.070372</td>\n      <td>0.061280</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.723533</td>\n      <td>5.687970</td>\n      <td>9.228014</td>\n      <td>7.410962</td>\n      <td>7.198935</td>\n      <td>11.340713</td>\n      <td>7.369151</td>\n      <td>10.589535</td>\n      <td>10.131034</td>\n      <td>3.039403</td>\n      <td>...</td>\n      <td>14.573446</td>\n      <td>10.166873</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.469047</td>\n      <td>7.855614</td>\n      <td>10.789010</td>\n      <td>9.705440</td>\n      <td>11.964504</td>\n      <td>14.177218</td>\n      <td>9.775521</td>\n      <td>12.749818</td>\n      <td>11.203400</td>\n      <td>4.147494</td>\n      <td>...</td>\n      <td>12.323687</td>\n      <td>5.084070</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.648386</td>\n      <td>5.412858</td>\n      <td>5.202138</td>\n      <td>6.334105</td>\n      <td>10.531077</td>\n      <td>8.599378</td>\n      <td>4.707143</td>\n      <td>6.276899</td>\n      <td>5.755446</td>\n      <td>8.934631</td>\n      <td>...</td>\n      <td>10.442652</td>\n      <td>4.127263</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.576799</td>\n      <td>1.618431</td>\n      <td>2.621299</td>\n      <td>1.711736</td>\n      <td>1.985173</td>\n      <td>2.103726</td>\n      <td>1.531987</td>\n      <td>2.888803</td>\n      <td>3.074646</td>\n      <td>3.813351</td>\n      <td>...</td>\n      <td>4.061364</td>\n      <td>3.374841</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>jackhammer</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 181 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Dataset Loader","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:57:10.783094Z","iopub.execute_input":"2025-06-09T19:57:10.783401Z","iopub.status.idle":"2025-06-09T19:57:10.869111Z","shell.execute_reply.started":"2025-06-09T19:57:10.783384Z","shell.execute_reply":"2025-06-09T19:57:10.868495Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"BATCH_SIZE = 32\nN_WORKERS = torch.cuda.device_count() if torch.cuda.device_count() > 1 else 1\nN_WORKERS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:57:10.870070Z","iopub.execute_input":"2025-06-09T19:57:10.870589Z","iopub.status.idle":"2025-06-09T19:57:10.905400Z","shell.execute_reply.started":"2025-06-09T19:57:10.870570Z","shell.execute_reply":"2025-06-09T19:57:10.904900Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"label_map = {\n    'air_conditioner': 0,\n    'car_horn': 1,\n    'children_playing': 2,\n    'dog_bark': 3,\n    'drilling': 4,\n    'engine_idling': 5,\n    'gun_shot': 6,\n    'jackhammer': 7,\n    'siren': 8,\n    'street_music': 9\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:57:10.905986Z","iopub.execute_input":"2025-06-09T19:57:10.906178Z","iopub.status.idle":"2025-06-09T19:57:10.909912Z","shell.execute_reply.started":"2025-06-09T19:57:10.906162Z","shell.execute_reply":"2025-06-09T19:57:10.909268Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class UrbanSound8kDataset(Dataset):\n    \"\"\" Diabetes dataset.\"\"\"\n\n    # Initialize your data, download, etc.\n    def __init__(self, file_path):\n        with open(file_path, 'rb') as f:\n            self.x_data, self.y_data = pickle.load(f)\n        self.len = len(self.x_data)\n        self.y_data = np.array([label_map[label] for label in self.y_data])\n\n    def __getitem__(self, idx):\n        x = self.x_data[idx].astype(np.float32)   # <- fix here\n        y = self.y_data[idx]\n        # print(type(x[0][0]))\n        # print(y)\n        # print(torch.tensor(y, dtype=torch.long))\n        return torch.from_numpy(x), torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return self.len\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:57:10.910594Z","iopub.execute_input":"2025-06-09T19:57:10.910773Z","iopub.status.idle":"2025-06-09T19:57:10.920252Z","shell.execute_reply.started":"2025-06-09T19:57:10.910753Z","shell.execute_reply":"2025-06-09T19:57:10.919761Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import numpy as np\nimport pickle\n\n# Load train data to compute stats\nwith open('/kaggle/input/urbansound8k-feature-extraction/train_data.pkl', 'rb') as f:\n    x_data, _ = pickle.load(f)\n\n# Stack to shape (num_samples * frames, features) = (N * 180, 55)\nx_all = np.vstack(x_data)\nmean = x_all.mean(axis=0)     # shape: (55,)\nstd = x_all.std(axis=0) + 1e-8  # Add epsilon to avoid divide-by-zero","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:57:10.922094Z","iopub.execute_input":"2025-06-09T19:57:10.922602Z","iopub.status.idle":"2025-06-09T19:57:13.777213Z","shell.execute_reply.started":"2025-06-09T19:57:10.922585Z","shell.execute_reply":"2025-06-09T19:57:13.776485Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_dataset = UrbanSound8kDataset('/kaggle/input/urbansound8k-feature-extraction/train_data.pkl')\ntrain_loader = DataLoader(dataset=train_dataset,\n                          batch_size=BATCH_SIZE,\n                          shuffle=True,\n                          num_workers=N_WORKERS)\nval_dataset = UrbanSound8kDataset('/kaggle/input/urbansound8k-feature-extraction/val_data.pkl')\nval_loader = DataLoader(dataset=val_dataset,\n                          batch_size=BATCH_SIZE,\n                          shuffle=True,\n                          num_workers=N_WORKERS)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:57:13.778034Z","iopub.execute_input":"2025-06-09T19:57:13.778266Z","iopub.status.idle":"2025-06-09T19:57:15.265994Z","shell.execute_reply.started":"2025-06-09T19:57:13.778243Z","shell.execute_reply":"2025-06-09T19:57:15.265410Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(len(train_dataset), len(train_dataset[0]), len(train_dataset[0][0]), len(train_dataset[0][0][0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:57:15.266687Z","iopub.execute_input":"2025-06-09T19:57:15.266894Z","iopub.status.idle":"2025-06-09T19:57:15.293928Z","shell.execute_reply.started":"2025-06-09T19:57:15.266877Z","shell.execute_reply":"2025-06-09T19:57:15.293364Z"}},"outputs":[{"name":"stdout","text":"21740 2 54 180\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(len(train_dataset), len(train_dataset[1]), train_dataset[1][1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:57:15.294573Z","iopub.execute_input":"2025-06-09T19:57:15.294759Z","iopub.status.idle":"2025-06-09T19:57:15.310790Z","shell.execute_reply.started":"2025-06-09T19:57:15.294745Z","shell.execute_reply":"2025-06-09T19:57:15.310301Z"}},"outputs":[{"name":"stdout","text":"21740 2 tensor(7)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(len(val_dataset), len(val_dataset[0]), len(val_dataset[0][0]), len(val_dataset[0][0][0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:57:15.311469Z","iopub.execute_input":"2025-06-09T19:57:15.311695Z","iopub.status.idle":"2025-06-09T19:57:15.316392Z","shell.execute_reply.started":"2025-06-09T19:57:15.311674Z","shell.execute_reply":"2025-06-09T19:57:15.315712Z"}},"outputs":[{"name":"stdout","text":"6576 2 54 180\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(len(val_dataset), len(val_dataset[1]), val_dataset[1][1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:57:15.317091Z","iopub.execute_input":"2025-06-09T19:57:15.317270Z","iopub.status.idle":"2025-06-09T19:57:15.328299Z","shell.execute_reply.started":"2025-06-09T19:57:15.317257Z","shell.execute_reply":"2025-06-09T19:57:15.327628Z"}},"outputs":[{"name":"stdout","text":"6576 2 tensor(7)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# RNN Model","metadata":{}},{"cell_type":"code","source":"class RNNClassifier(nn.Module):\n    # Our model\n\n    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n        super(RNNClassifier, self).__init__()\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.n_directions = int(bidirectional) + 1\n\n        self.projection = nn.Linear(input_size, hidden_size)\n        self.rnn1 = nn.RNN(hidden_size, hidden_size, n_layers,\n                          bidirectional=bidirectional, batch_first=True)\n        # self.rnn2 = nn.RNN(hidden_size * self.n_directions, hidden_size, n_layers,\n        #                   bidirectional=bidirectional, batch_first=True)\n        self.fc1 = nn.Linear(hidden_size * self.n_directions, hidden_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.2)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, signal):\n        # Note: we run this all at once (over the whole input sequence)\n        # input shape: B x S (input size)\n        # transpose to make S(sequence) x B (batch)\n        # input = input.t()\n        batch_size = signal.size(0)\n        signal = signal.permute(0, 2, 1)\n\n        # Make a hidden\n        hidden = self._init_hidden(batch_size, signal.device)\n        # print(\"hidden shape: \", hidden.shape)\n        # print(\"signal shape: \", signal.shape)\n        # # Embedding S x B -> S x B x I (embedding size)\n        # embedded = self.embedding(input)\n\n        # # Pack them up nicely\n        # gru_input = pack_padded_sequence(\n        #     embedded, seq_lengths.data.cpu().numpy())\n\n        # To compact weights again call flatten_parameters().\n        # self.gru.flatten_parameters()\n        output, hidden = self.rnn1(self.projection(signal), hidden)\n        # print(\"output shape: \", output.shape)\n        # print(\"hidden shape: \", hidden.shape)\n        # output, hidden = self.rnn2(output, hidden)\n\n        if self.n_directions == 2:\n            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n        else:\n            hidden = hidden[-1]\n        \n        # print(\"hidden shape: \", hidden.shape)\n        # New\n        hidden = self.fc1(hidden)\n        hidden = self.relu(hidden)\n        hidden = self.dropout(hidden)\n        \n        # Use the last layer output as FC's input\n        # No need to unpack, since we are going to use hidden\n        fc_output = self.fc2(hidden)\n        # print(\"fc_output shape: \", fc_output.shape)\n        \n        return fc_output\n\n    def _init_hidden(self, batch_size, device):\n        hidden = torch.zeros(self.n_layers * self.n_directions,\n                             batch_size, self.hidden_size).to(device)\n        return create_variable(hidden)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:20:48.185825Z","iopub.execute_input":"2025-06-09T20:20:48.186390Z","iopub.status.idle":"2025-06-09T20:20:48.194025Z","shell.execute_reply.started":"2025-06-09T20:20:48.186368Z","shell.execute_reply":"2025-06-09T20:20:48.193296Z"}},"outputs":[],"execution_count":154},{"cell_type":"code","source":"class RNNClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, n_layers=2, bidirectional=True):\n        super(RNNClassifier, self).__init__()\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.n_directions = 2 if bidirectional else 1\n        self.bidirectional = bidirectional\n\n        self.projection = nn.Linear(input_size, hidden_size)\n        self.rnn = nn.GRU(hidden_size, hidden_size, n_layers,\n                          bidirectional=bidirectional, batch_first=True)\n        \n        self.fc1 = nn.Linear(hidden_size * self.n_directions, hidden_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.2)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x = x.permute(0, 2, 1)  # Make it [B, S, I]\n\n        \n        h0 = torch.zeros(self.n_layers * self.n_directions, batch_size, self.hidden_size).to(x.device)\n        out, hn = self.rnn(self.projection(x), h0)\n\n        if self.bidirectional:\n            hn = torch.cat((hn[-2], hn[-1]), dim=1)\n        else:\n            hn = hn[-1]\n\n        x = self.fc1(hn)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:26:09.919420Z","iopub.execute_input":"2025-06-09T20:26:09.919925Z","iopub.status.idle":"2025-06-09T20:26:09.927053Z","shell.execute_reply.started":"2025-06-09T20:26:09.919897Z","shell.execute_reply":"2025-06-09T20:26:09.926493Z"}},"outputs":[],"execution_count":174},{"cell_type":"code","source":"# Train cycle\ndef train():\n    total_loss = 0\n\n    for i, (signal, label) in enumerate(train_loader, 1):\n        output = classifier(signal)\n        # print(\"signal size: \", signal.shape)\n        # print(\"label size: \", label.shape)\n        # print(\"label: \", label)\n        loss = criterion(output, label)\n        total_loss += loss.item()\n\n        classifier.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if i % 100 == 0:\n            print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f}'.format(\n                time_since(start), epoch,  i *\n                len(signal), len(train_loader.dataset),\n                100. * i * len(signal) / len(train_loader.dataset),\n                total_loss / i * len(signal)))\n\n    return total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:58:02.571968Z","iopub.execute_input":"2025-06-09T19:58:02.572210Z","iopub.status.idle":"2025-06-09T19:58:02.578387Z","shell.execute_reply.started":"2025-06-09T19:58:02.572193Z","shell.execute_reply":"2025-06-09T19:58:02.577474Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def train():\n    classifier.train()\n    total_loss = 0\n\n    for i, (signal, label) in enumerate(train_loader, 1):\n        signal = signal.to(device)\n        label = label.to(device)\n\n        output = classifier(signal)\n        loss = criterion(output, label)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * signal.size(0)\n\n        if i % 100 == 0:\n            avg_loss = total_loss / (i * signal.size(0))\n            print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n                time_since(start), epoch, i * len(signal),\n                len(train_loader.dataset),\n                100. * i * len(signal) / len(train_loader.dataset),\n                avg_loss))\n\n    return total_loss / len(train_loader.dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:58:02.809798Z","iopub.execute_input":"2025-06-09T19:58:02.810548Z","iopub.status.idle":"2025-06-09T19:58:02.815919Z","shell.execute_reply.started":"2025-06-09T19:58:02.810524Z","shell.execute_reply":"2025-06-09T19:58:02.815206Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Testing cycle\ndef test(name=None):\n    \n    print(\"evaluating trained model ...\")\n    correct = 0\n    total_loss = 0\n    train_data_size = len(val_loader.dataset)\n\n    for signal, label in val_loader:\n        output = classifier(signal)\n        pred = output.data.max(1, keepdim=True)[1]\n        total_loss += criterion(output, label)\n        correct += pred.eq(label.data.view_as(pred)).cpu().sum()\n\n    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        correct, train_data_size, 100. * correct / train_data_size))\n    print(f\"Loss: {total_loss / len(val_loader.dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:41:10.363334Z","iopub.execute_input":"2025-06-09T20:41:10.364044Z","iopub.status.idle":"2025-06-09T20:41:10.368917Z","shell.execute_reply.started":"2025-06-09T20:41:10.364020Z","shell.execute_reply":"2025-06-09T20:41:10.368167Z"}},"outputs":[],"execution_count":198},{"cell_type":"code","source":"def test(name=None):\n    classifier.eval()\n    correct = 0\n    total = 0\n    total_loss = 0\n\n    with torch.no_grad():\n        for signal, label in val_loader:\n            signal = signal.to(device)\n            label = label.to(device)\n            \n\n            output = classifier(signal)\n            total_loss += criterion(output, label).item() * signal.size(0)\n            pred = output.argmax(dim=1)\n            correct += (pred == label).sum().item()\n            total += label.size(0)\n\n    accuracy = 100. * correct / total\n    print('\\nTest set: Accuracy: {}/{} ({:.2f}%)\\n'.format(correct, total, accuracy))\n    print(f\"Loss: {total_loss / len(val_loader.dataset)}\\n\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:44:10.334535Z","iopub.status.idle":"2025-06-09T20:44:10.334737Z","shell.execute_reply.started":"2025-06-09T20:44:10.334635Z","shell.execute_reply":"2025-06-09T20:44:10.334646Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Some utility functions","metadata":{}},{"cell_type":"code","source":"def create_variable(tensor):\n    # Do cuda() before wrapping with variable\n    if torch.cuda.is_available():\n        return Variable(tensor.cuda())\n    else:\n        return Variable(tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:58:04.895297Z","iopub.execute_input":"2025-06-09T19:58:04.895563Z","iopub.status.idle":"2025-06-09T19:58:04.899453Z","shell.execute_reply.started":"2025-06-09T19:58:04.895542Z","shell.execute_reply":"2025-06-09T19:58:04.898748Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def time_since(since):\n    s = time.time() - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T19:58:05.098023Z","iopub.execute_input":"2025-06-09T19:58:05.098217Z","iopub.status.idle":"2025-06-09T19:58:05.101908Z","shell.execute_reply.started":"2025-06-09T19:58:05.098201Z","shell.execute_reply":"2025-06-09T19:58:05.101288Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"# Model Training & Valuation","metadata":{}},{"cell_type":"code","source":"N_EPOCHS = 100\n\nN_INPUT = 54\nHIDDEN_SIZE = 180\nN_CLASSES = 10\nN_LAYERS = 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:20:53.044474Z","iopub.execute_input":"2025-06-09T20:20:53.045214Z","iopub.status.idle":"2025-06-09T20:20:53.048442Z","shell.execute_reply.started":"2025-06-09T20:20:53.045188Z","shell.execute_reply":"2025-06-09T20:20:53.047882Z"}},"outputs":[],"execution_count":155},{"cell_type":"code","source":"N_EPOCHS = 100\n\nN_INPUT = 54\nHIDDEN_SIZE = 256\nN_CLASSES = 10\nN_LAYERS = 2\n\nclassifier = RNNClassifier(\n    input_size=N_INPUT,\n    hidden_size=HIDDEN_SIZE,\n    output_size=N_CLASSES,\n    n_layers=N_LAYERS,\n    # bidirectional=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:40:55.842420Z","iopub.execute_input":"2025-06-09T20:40:55.843137Z","iopub.status.idle":"2025-06-09T20:40:55.865519Z","shell.execute_reply.started":"2025-06-09T20:40:55.843108Z","shell.execute_reply":"2025-06-09T20:40:55.865051Z"}},"outputs":[],"execution_count":193},{"cell_type":"code","source":"if torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    # dim = 0 [33, xxx] -> [11, ...], [11, ...], [11, ...] on 3 GPUs\n    classifier = nn.DataParallel(classifier)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:40:56.054058Z","iopub.execute_input":"2025-06-09T20:40:56.054229Z","iopub.status.idle":"2025-06-09T20:40:56.058417Z","shell.execute_reply.started":"2025-06-09T20:40:56.054216Z","shell.execute_reply":"2025-06-09T20:40:56.057807Z"}},"outputs":[{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"}],"execution_count":194},{"cell_type":"code","source":"if torch.cuda.is_available():\n    classifier.cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:40:56.243534Z","iopub.execute_input":"2025-06-09T20:40:56.243700Z","iopub.status.idle":"2025-06-09T20:40:56.251985Z","shell.execute_reply.started":"2025-06-09T20:40:56.243687Z","shell.execute_reply":"2025-06-09T20:40:56.251400Z"}},"outputs":[],"execution_count":195},{"cell_type":"code","source":"test()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:44:13.156555Z","iopub.execute_input":"2025-06-09T20:44:13.156827Z","iopub.status.idle":"2025-06-09T20:44:16.691998Z","shell.execute_reply.started":"2025-06-09T20:44:13.156806Z","shell.execute_reply":"2025-06-09T20:44:16.691125Z"}},"outputs":[{"name":"stdout","text":"\nTest set: Accuracy: 3951/6576 (60.08%)\n\nLoss: 1.757803818025148\n","output_type":"stream"}],"execution_count":207},{"cell_type":"code","source":"optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\nstart = time.time()\nprint(\"Training for %d epochs...\" % N_EPOCHS)\nfor epoch in range(1, N_EPOCHS + 1):\n    # Train cycle\n    train()\n\n    # Testing\n    test()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T20:44:53.477252Z","iopub.execute_input":"2025-06-09T20:44:53.477702Z","iopub.status.idle":"2025-06-09T20:44:54.719048Z","shell.execute_reply.started":"2025-06-09T20:44:53.477678Z","shell.execute_reply":"2025-06-09T20:44:54.717980Z"}},"outputs":[{"name":"stdout","text":"Training for 100 epochs...\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_35/705636435.py\", line 8, in <cell line: 0>\n    train()\n  File \"/tmp/ipykernel_35/983827614.py\", line 13, in train\n    loss.backward()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 626, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n    stb = value._render_traceback_()\n          ^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n    traceback_info = getframeinfo(tb, context)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 1684, in getframeinfo\n    filename = getsourcefile(frame) or getfile(frame)\n               ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n    module = getmodule(object, filename)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/inspect.py\", line 988, in getmodule\n    if ismodule(module) and hasattr(module, '__file__'):\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/705636435.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Train cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/983827614.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"],"ename":"TypeError","evalue":"object of type 'NoneType' has no len()","output_type":"error"}],"execution_count":209},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}